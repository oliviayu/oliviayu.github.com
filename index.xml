<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tingting&#39;s Blog on Tingting&#39;s Blog</title>
    <link>/</link>
    <description>Recent content in Tingting&#39;s Blog on Tingting&#39;s Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Apr 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>/post/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Skills</title>
      <link>/author/skills/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/author/skills/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>/experience/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/experience/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent Posts</title>
      <link>/recent_posts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/recent_posts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/project/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>/talk/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/talk/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/author/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/author/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>sparklyr (Spark in R)</title>
      <link>/post/2019-04-10-sparklyr/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-04-10-sparklyr/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; programming language, along with &lt;code&gt;RStudio&lt;/code&gt;, has become one of the most popular tools for data analysis as it contains a large amount of open-source packages developed by a community of statisticians. However, &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;RStudio&lt;/code&gt; is not ideal for Big Data analysis as mostly the data would not fit into R memory. On the other hand, Spark has become the leading platform for big-data analytics. It works with the system to distribute data across clusters and process data in parallel. Moreover it provides native bindings for different languages such as Java, Python, Scala, and R.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sparklyr&lt;/code&gt; is an R package that allows us to analyze data in Spark from R. It supports &lt;code&gt;dplyr&lt;/code&gt;, a popular tool for working with data frame like objects both in memory and out of memory, and many machine learning algorithms to run classifiers, regressions, and so on in Spark. It is extensible that you can create R packages that depend on &lt;code&gt;sparklyr&lt;/code&gt; to call the full Spark API, such as H2O’s &lt;code&gt;rsparkling&lt;/code&gt;, an R package that works with H2O’s machine learning algorithm. With &lt;code&gt;sparklyr&lt;/code&gt; and &lt;code&gt;rsparkling&lt;/code&gt;, we have access to all the tools in H2O for analysis with R and Spark.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connect-to-spark&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Connect to Spark&lt;/h1&gt;
&lt;p&gt;Suppose that &lt;code&gt;sparklyr&lt;/code&gt; has been successfully installed in your &lt;code&gt;R&lt;/code&gt; environment. To get start with Spark using &lt;code&gt;sparklyr&lt;/code&gt; and a local cluster,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sparklyr)
spark_install()
sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or if a Spark cluster has been made available to you&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sc &amp;lt;- spark_connect(master = &amp;quot;&amp;lt;cluster-master&amp;gt;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I run &lt;code&gt;spark_connect(master = &amp;quot;local&amp;quot;)&lt;/code&gt;, I got the error message&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Java 9 is currently unsupported in Spark distributions unless you manually install Hadoop 2.8 and manually configure Spark. Please consider uninstalling Java 9 and reinstalling Java 8. To override this failure set &amp;#39;options(sparklyr.java9 = TRUE)&amp;#39;.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s because on my Windows 10 laptop, my JAVA_HOME was set to &lt;code&gt;C:\Java\jdk&lt;/code&gt;, which has Version 11, in the system environment. I change it to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;JAVA_HOME = &amp;quot;C:\Java\jre1.8.0_151&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that my Java folder was in &lt;code&gt;C:\Program Files (x86)\..&lt;/code&gt; and it created an issue when connecting to Spark. So I moved the folder directly to &lt;code&gt;C:\..&lt;/code&gt; to solve the problem. Howoever, another error triggers when connecting to Spark&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---- Output Log ----
Error occurred during initialization of VM
Could not reserve enough space for 2097152KB object heap&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It cannot allocate 2GB and this seems a common issue under Windows with a Java version using x86. To solve this, we can either install java x64 or reduce the default memory&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;config &amp;lt;- spark_config()
config[[&amp;quot;sparklyr.shell.driver-memory&amp;quot;]] &amp;lt;- &amp;quot;512m&amp;quot;
sc &amp;lt;- spark_connect(master = &amp;quot;local&amp;quot;, config = config)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I get connected to Spark! Now I can run analyses and build models using Spark from R.&lt;/p&gt;
&lt;p&gt;To monitor and analyze execution, we can go to the Spark’s web interface:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spark_web(sc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we are done with analysis, we can disconnect spark,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spark_disconnect(sc)&lt;/code&gt;&lt;/pre&gt;
&lt;!--I&#39;m using a local cluster with Spark 2.4.0 and Hadoop 2.7 on Windows 10. However my computer has Java 11 installed and it gives me an error when running
This means I have to either install Hadoop 2.8 or later or rollback Java to version 8. Emmm... 
I chose the first option and installed Hadoop 2.9.1 in my computer, following the well written instructions by Parixit Odedara (https://exitcondition.com/install-hadoop-windows/). To unpack the compressed files of hadoop, I use the `tar` command in git bash 
```
tar xvf hadoop-2.9.1.tar.gz
```
When editing the PATH in the system environment setting, 
```
%JAVA_HOME%
%HADOOP_HOME%
%HADOOP_BIN%
%HADOOP_HOME%\sbin
```
didn&#39;t work out for me, so I just set the complete paths in the PATH
```
C:\Program Files (x86)\Java\jdk
C:\hadoop-2.9.1
C:\hadoop-2.9.1\bin
C:\hadoop-2.9.1\sbin
```
Similarly in the `etc\hadoop\hadoop-env.cmd` file, I set 
```
set JAVA_HOME=&#34;C:\Program Files (x86)\Java\jdk&#34;
```
--&gt;
&lt;!--
To check which versions are available or installed:
```r
spark_available_versions()
spark_installed_versions()
```
--&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Analysis&lt;/h1&gt;
&lt;div id=&#34;copy-data-to-spark&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Copy data to Spark&lt;/h2&gt;
&lt;p&gt;The data set &lt;code&gt;mtcars&lt;/code&gt; is a dataframe available in &lt;code&gt;R&lt;/code&gt;. Run &lt;code&gt;?mtcars&lt;/code&gt; to see more details. To copy the data set into Apache Spark&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cars &amp;lt;- copy_to(sc, mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can access the data that was copied into Spark from R using the &lt;code&gt;cars&lt;/code&gt; reference.&lt;/p&gt;
&lt;p&gt;To read data from existing data sources in csv format and copy to Spark,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cars &amp;lt;- spark_read_csv(sc, &amp;quot;cars.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To export data as a csv file,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spark_write_csv(cars, &amp;quot;cars.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other formats like plain text, JSON, JDBC are supported as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;p&gt;When using Spark from R to analyze data, most regular &lt;code&gt;R&lt;/code&gt; functions, such as &lt;code&gt;nrow&lt;/code&gt;, won’t work directly on the Spark reference &lt;code&gt;cars&lt;/code&gt;. Instead, we can either use SQL through the &lt;code&gt;DBI&lt;/code&gt; package or use &lt;code&gt;dplyr&lt;/code&gt; (strongly preferred). Most of the data transformation made available by &lt;code&gt;dplyr&lt;/code&gt; to work with local data frames are also available to use with a Spark connection. This means that a general approach to learning &lt;code&gt;dplyr&lt;/code&gt; can be taken in order to gain more proficiency with data exploration and preparation with Spark. For example, to count how many records are available in &lt;code&gt;cars&lt;/code&gt;,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dplyr::count(cars)  # = nrow(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To select columns, sample rows, and collect data from Spark,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_in_r &amp;lt;- dplyr::select(cars, hp, mpg) %&amp;gt;% 
             dplyr::sample_n(100) %&amp;gt;% 
             dplyr::collect()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can apply regular &lt;code&gt;R&lt;/code&gt; functions on the dataframe &lt;code&gt;df_in_r&lt;/code&gt;, for example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(df_in_r)
plot(df_in_r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a particular functionality is not available in Spark and no extension has been developed, we can distribute the R code across the Spark cluster. For example,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cars %&amp;gt;% spark_apply(nrow)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a powerful tools but comes with additional complexity that we should only use as a last resort option. We should learn how to do proper data analysis and modeling without having to distribute custom R code across our cluster!&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;corrr&lt;/code&gt; package specializes in correlations. It contains friendly functions to prepare and visualize the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(corrr)
cars %&amp;gt;%
  correlate(use = &amp;quot;pairwise.complete.obs&amp;quot;, method = &amp;quot;pearson&amp;quot;) %&amp;gt;% 
  shave() %&amp;gt;%
  rplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sparklyr&lt;/code&gt; package also provides some functions for data transformation and exploratory data analysis. Those functions usually have &lt;code&gt;sdf_&lt;/code&gt; as a prefix.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling&lt;/h2&gt;
&lt;p&gt;Spark MLlib is the component of Spark that allows one to write high level code to perform machine learning tasks on distributed data. Sparklyr provides an interface to the ML algorithms that should be familiar to R users. For example, you can run a linear regression as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- ml_linear_regression(cars, mpg ~ hp)
model %&amp;gt;%
  ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %&amp;gt;%
  transmute(hp = hp, mpg = prediction) %&amp;gt;%
  full_join(select(cars, hp, mpg)) %&amp;gt;%
  collect() %&amp;gt;%
  plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To retrieve additional statistics from the model,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::glance(model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spark provides a wide range of algorithms and feature transformers. Those functions usually have &lt;code&gt;ml_&lt;/code&gt; or &lt;code&gt;ft_&lt;/code&gt; as prefix.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://therinspark.com/starting.html#starting-spark-web-interface&#34; class=&#34;uri&#34;&gt;https://therinspark.com/starting.html#starting-spark-web-interface&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ARIMA</title>
      <link>/post/2019-04-03-arima/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-04-03-arima/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The ARIMA (autoregressive integrated moving average) models are also known as &lt;strong&gt;Box–Jenkins models&lt;/strong&gt;. ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the “integrated” part of the model) can be applied one or more times to eliminate the non-stationarity.&lt;/p&gt;
&lt;p&gt;The AR part of ARIMA indicates that the evolving variable of interest is regressed on its own lagged (i.e., prior) values. The MA part indicates that the regression error is actually a linear combination of error terms whose values occurred contemporaneously and at various times in the past. The I (for “integrated”) indicates that the data values have been replaced with the difference between their values and the previous values (and this differencing process may have been performed more than once).&lt;/p&gt;
&lt;p&gt;When two out of the three model parameters are zeros, the model may be referred to based on the non-zero parameter, dropping “AR”, “I” or “MA” from the acronym describing the model. For example, &lt;span class=&#34;math inline&#34;&gt;\(ARIMA (1,0,0)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(AR(1)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,1,0)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(I(1)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,0,1)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(MA(1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arp&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;AR(p)&lt;/h1&gt;
&lt;p&gt;An autoregressive (AR) model is a representation of a type of random process. The AR model specifies that the output variable depends &lt;em&gt;linearly&lt;/em&gt; on its own previous values and on a stochastic term. In general, the AR model of order &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(AR(p)\)&lt;/span&gt;, is defined as &lt;span class=&#34;math display&#34;&gt;\[y_t = c + \sum_{i=1}^p \phi_i y_{t-i} +\epsilon_t,\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\phi_1,\cdots,\phi_p\)&lt;/span&gt; are the parameters, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is a constant, and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\)&lt;/span&gt; is white noise often assumed following &lt;span class=&#34;math inline&#34;&gt;\(N(0,\sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For AR(1), &lt;span class=&#34;math inline&#34;&gt;\(|\phi|&amp;lt;1\)&lt;/span&gt; is necessary for the process to be stationary, such that &lt;span class=&#34;math display&#34;&gt;\[E(y_t) = E(y_{t-1}) = \mu,\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[\text{var}(y_t) = \text{var}(y_{t-1})=\frac{\sigma^2}{1-\phi^2} = \sigma_y^2,\]&lt;/span&gt; assuming &lt;span class=&#34;math inline&#34;&gt;\(y_{t}\)&lt;/span&gt;’s and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\)&lt;/span&gt;’s are independent from each other. The condition &lt;span class=&#34;math inline&#34;&gt;\(|\phi|&amp;lt;1\)&lt;/span&gt; appears in the variance term so that &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y^2\)&lt;/span&gt; is finite and positive. Note that &lt;span class=&#34;math inline&#34;&gt;\(\text{var}(y_t)\)&lt;/span&gt; in AR(1) is larger than in AR(0) i.e. regular linear models without autoregressions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;id&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;I(d)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Differencing&lt;/strong&gt; in statistics is a transformation applied to time-series data in order to make it stationary. Differencing removes the changes in the level of a time series, eliminating trend and seasonality and consequently stabilizing the mean of the time series. The differenced data is then used for the estimation of an ARMA model.&lt;/p&gt;
&lt;p&gt;The I(1) model of first-order differencing can be written as &lt;span class=&#34;math display&#34;&gt;\[D(y_t) = y_t - y_{t-1} = \epsilon_t,\]&lt;/span&gt; and the I(2) model of second-order differencing can be written as &lt;span class=&#34;math display&#34;&gt;\[D^2(y_t)  = D(y_t) - D(y_{t-1})  = y_t - 2y_{t-1}+y_{t-2} = \epsilon_t,\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(D()\)&lt;/span&gt; is the operator of differencing and &lt;span class=&#34;math inline&#34;&gt;\(D^d(y_t) = D^{d-1}(y_t) - D^{d-1}(y_{t-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another method of differencing data is &lt;strong&gt;seasonal differencing&lt;/strong&gt;, which involves computing the difference between an observation and the corresponding observation in the previous period. For example, &lt;span class=&#34;math display&#34;&gt;\[y&amp;#39;_t = y_t - y_{t-s},\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is the duration of season. We denote it as &lt;span class=&#34;math inline&#34;&gt;\(D_s(y_t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maq&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;MA(q)&lt;/h1&gt;
&lt;p&gt;The moving average model of order &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(MA(q)\)&lt;/span&gt;, is given as &lt;span class=&#34;math display&#34;&gt;\[y_t = \mu + \epsilon_t + \sum_{i=1}^q\theta_i\epsilon_{t-i},\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\theta_1,\cdots,\theta_q\)&lt;/span&gt; are the parameters, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the expectation of &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t, \epsilon_{t-1},\cdots\)&lt;/span&gt; are white noise error terms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arimapdq&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ARIMA(p,d,q)&lt;/h1&gt;
&lt;div id=&#34;non-seasonal-arima&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Non-seasonal ARIMA&lt;/h2&gt;
&lt;p&gt;Non-seasonal ARIMA models are generally denoted &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(p,d,q)\)&lt;/span&gt; where parameters &lt;span class=&#34;math inline&#34;&gt;\(p, d, q\)&lt;/span&gt; are non-negative integers, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the order (number of time lags) of the autoregressive model, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the degree of differencing (the number of times the data have had past values subtracted), and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the order of the moving-average model. In general, an &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(p,d, q)\)&lt;/span&gt; model is given as &lt;span class=&#34;math display&#34;&gt;\[D^d(y_t) = \delta + \sum_{i=1}^p\phi_i D^d(y_{t-i}) + \epsilon_t +
\sum_{i=1}^q \theta_i \epsilon_{t-i},\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(D^d(y_t)\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-order difference of &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonal-arima&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Seasonal ARIMA&lt;/h2&gt;
&lt;p&gt;Seasonal ARIMA models are usually denoted &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(p,d,q)\times(P,D,Q)_s\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; refers to the time span of repeating seasonal pattern, and the uppercase &lt;span class=&#34;math inline&#34;&gt;\(P,D,Q\)&lt;/span&gt; refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model.&lt;/p&gt;
&lt;p&gt;The non-seasonal components are the same as in non-seasonal ARIMA. As for the seasonal components, we have, for example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Seasonal AR: &lt;span class=&#34;math inline&#34;&gt;\(y_t = c + \sum_{i=1}^P\psi_i y_{t-si}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Seasonal MA: &lt;span class=&#34;math inline&#34;&gt;\(y_t = \mu+ \epsilon_t + \sum_{i=1}^Q\eta_i\epsilon_{t-si}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Seasonal I: &lt;span class=&#34;math inline&#34;&gt;\(D^D_s(y_t) = D^{D-1}_s(y_t)- D_s^{D-1}(y_{t-s})\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Some well-known special cases arise naturally or are mathematically equivalent to other popular forecasting models. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,1,0)\)&lt;/span&gt; model (or &lt;span class=&#34;math inline&#34;&gt;\(I(1)\)&lt;/span&gt; model) is given by &lt;span class=&#34;math inline&#34;&gt;\(X_{t}=X_{t-1}+\epsilon_{t}\)&lt;/span&gt;, which is simply a random walk.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,1,0)\)&lt;/span&gt; with a constant, given by &lt;span class=&#34;math inline&#34;&gt;\(X_{t}=c+X_{t-1}+\epsilon_{t}\)&lt;/span&gt;, which is a random walk with drift.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,0,0)\)&lt;/span&gt; model is a white noise model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,1,2)\)&lt;/span&gt; model is a Damped Holt’s model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,1,1)\)&lt;/span&gt; model without constant is a &lt;strong&gt;basic exponential smoothing model&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(0,2,2)\)&lt;/span&gt; model is given by &lt;span class=&#34;math inline&#34;&gt;\(X_{t}=2X_{t-1}-X_{t-2}+(\alpha +\beta -2)\epsilon_{t-1}+(1-\alpha )\epsilon_{t-2}+\epsilon_{t}\)&lt;/span&gt;, which is equivalent to Holt’s linear method with additive errors, or &lt;strong&gt;second-order exponential smoothing&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Structural Time-Series Models</title>
      <link>/post/2019-03-21-bsts/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-03-21-bsts/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;State-space models&lt;/strong&gt; were originally developed by control engineers, particularly for applications that require continuous updating of the current position. An example, from the field of navigation systems, is updating an user equipment’s position. The models have also found increasing use in many types of time-series problems, including parameter estimation, smoothing, and prediction. &lt;strong&gt;Structural time-series models&lt;/strong&gt; are state-space models for time-series data. They are useful in practice because they are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;flexible&lt;/strong&gt; : a very large class of models can be expressed in state space forms, including all ARIMA and VARMA models;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;modular&lt;/strong&gt; : the model can be assembled from a library of state-component sub-models to capture important features of the data. Several widely used &lt;strong&gt;state components&lt;/strong&gt; are available for capturing the trend, seasonality, or effects of holidays.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;bsts&lt;/em&gt; R package is a tool for fitting structural time series models using Bayesian methods and &lt;em&gt;bsts&lt;/em&gt; stands for Bayesian structural time series. The &lt;em&gt;bsts&lt;/em&gt; can be configured for short term or long term forecasting, incorporating one or more seasonal effects, or fitting explanatory models if forecasting is not the primary goal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-form&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General Form&lt;/h1&gt;
&lt;p&gt;A general form of (univariate) structural time-series model &lt;!-- which underlies the Kalman filter--&gt; can be written as &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = Z_t^T \alpha_t + \epsilon_t, \qquad (\text{observation equation})\\
&amp;amp; \alpha_{t+1} = T_t\alpha_{t} + R_t\eta_t, \qquad (\text{transition or state equation})\\
&amp;amp; \epsilon_t\sim N(0,\sigma^2_t),\quad \eta_t \sim N(0, Q_t)
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is the observed value of a time series at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional &lt;strong&gt;state vector&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;-dimensional &lt;strong&gt;output vector&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_t\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m\times m\)&lt;/span&gt; &lt;strong&gt;transition matrix&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(R_t\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(m\times q\)&lt;/span&gt; &lt;strong&gt;control matrix&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\)&lt;/span&gt; is a scalar observation error, and &lt;span class=&#34;math inline&#34;&gt;\(\eta_t\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;-dimensional system error with a &lt;span class=&#34;math inline&#34;&gt;\(q\times q\)&lt;/span&gt; &lt;strong&gt;state-diffusion matrix&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Q_t\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(q\leq m\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The observation equation links the observed data &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; with the unobserved latent state &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The state vector &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt; is of prime importance and is usually unobserved or partially known. Although it may not be directly observable, it is often reasonable to assume that we know how it changes over time, and we denote the updating equation by the transition or state equation above. This equation defines how the latent space evolves over time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The arrays &lt;span class=&#34;math inline&#34;&gt;\(Z_t, T_t, R_t\)&lt;/span&gt; typically contain a mix of known values (often 0 and 1) and unknown parameters. The 0’s and 1’s indicate which bits of &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt; are relevant for a particular computation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta_t\)&lt;/span&gt; are generally assumed to be serially uncorrelated and also to be uncorrelated with each other at all time periods. In practice, we often assume &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\sim N(0,\sigma_t^2)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The term &lt;span class=&#34;math inline&#34;&gt;\(R_t\eta_t\)&lt;/span&gt; allows us to incorporate state components of less than full ranks. A model for seasonality will be the most important example.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The analyst chooses the structure of &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt; based on the specific data and task, such as whether it is for short or long term forecast, whether the data contains seasonal effects, and whether and how regressors are to be included. Many of these models are standard, and can be fit using a variety of tools, such as the &lt;em&gt;StructTS&lt;/em&gt; function distributed with base R or one of several R packages for fitting these models, such as the &lt;em&gt;dlm&lt;/em&gt; package for dynamic linear model. The &lt;em&gt;bsts&lt;/em&gt; package handles all the standard cases, but it also includes several useful extensions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;state-components&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;State Components&lt;/h1&gt;
&lt;div id=&#34;static-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Static intercept&lt;/h2&gt;
&lt;p&gt;We can add a static intercept term to a model, &lt;span class=&#34;math display&#34;&gt;\[y_t = c+\epsilon_t,\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is a constant value. If the structural time-series model includes a traditional trend component (e.g. local level, local linear trend, etc) then a separate intercept is not needed (and will probably cause trouble, as it will be confounded with the initial state of the trend model). However, if there is no trend, or the trend is an AR process centered around zero, then adding a static intercept will shift the center to a data-determined value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddStaticIntercept(ss, y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend&lt;/h2&gt;
&lt;div id=&#34;local-level&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Local level&lt;/h3&gt;
&lt;p&gt;The local level model assumes the trend is a random walk: &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp;y_t = \mu_t +\epsilon_t,\\
&amp;amp;\mu_{t+1} =  \mu_{t} + \eta_{t}
\end{split}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddLocalLevel(ss, y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;AR&lt;/h3&gt;
&lt;p&gt;An autoregressive (AR) model is a representation of a type of random process. The AR model specifies that the time series &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; depends &lt;em&gt;linearly&lt;/em&gt; on its own previous values and on a stochastic term. In general, the AR model of order &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, i.e. &lt;span class=&#34;math inline&#34;&gt;\(AR(p)\)&lt;/span&gt;, can be written as &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp;y_t = \mu_t +\epsilon_t,\\
&amp;amp;\mu_{t+1} = \sum_{i=0}^{p-1} \phi_i \mu_{t-i} + \eta_{t}
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\phi_0,\cdots,\phi_{p-1}\)&lt;/span&gt; are the parameters of the model. &lt;!--Note that when $p=1$ and $\phi_0=1$, the AR process is equivalent to the local level component. It can be re-written in the general form with ???
--&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddAr(ss, y, lags = p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;bsts&lt;/em&gt; package also supports sparse AR(p) process for large &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, where a spike and slab prior is applied on the autoregression coefficients &lt;span class=&#34;math inline&#34;&gt;\(\phi_0,\cdots, \phi_{p-1}\)&lt;/span&gt;. This model differs from the one in &lt;code&gt;AddAr()&lt;/code&gt; only in that some of its coefficients may be set to zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddAutoAr(ss, y, lags = p)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;local-linear-trend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Local linear trend&lt;/h3&gt;
&lt;p&gt;The local linear trend is a popular choice for modelling trends because it quickly adapts to local variation, which is desirable when making short-term predictions. However, this degree of flexibility may not be desired when making long-term predictions, as such predictions often come with implausibly wide uncertainty intervals.&lt;/p&gt;
&lt;p&gt;A local linear trend model for &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; can be written as, &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = \mu_t +\epsilon_t, \\
&amp;amp; \mu_{t+1} = \mu_{t} + \delta_t + \eta_{\mu,t}, \qquad \text{(stochastic level component)}\\
&amp;amp; \delta_{t+1} = \delta_t + \eta_{\delta,t}, \qquad \text{(stochastic slope component)}\\
&amp;amp; \eta_{\mu, t} \sim N(0,\sigma^2_{\mu, t}),\quad
  \eta_{\delta, t} \sim N(0,\sigma^2_{\delta, t})
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt; is the value of the trend at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\delta_t\)&lt;/span&gt; is the expected increase in &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; between time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt; so it can be thought of as the slope at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\eta_{\mu,t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta_{\delta, t}\)&lt;/span&gt; are error terms independent from each other. A local linear trend allows both level (&lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt;) and slope (&lt;span class=&#34;math inline&#34;&gt;\(\delta_t\)&lt;/span&gt;) to be stochastic. It assumes that both the level and the slope follow random walks.&lt;/p&gt;
&lt;p&gt;The model can also be expressed in the general form &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = [1\quad 0]\left[\begin{matrix}\mu_t\\\delta_t\end{matrix}\right] +\epsilon_t, \\
&amp;amp; \left[\begin{matrix}\mu_{t+1}\\\delta_{t+1}\end{matrix}\right] = 
\left[\begin{matrix}1 &amp;amp; 1\\ 0 &amp;amp; 1 \end{matrix}\right]
\left[\begin{matrix}\mu_t\\\delta_t\end{matrix}\right] + 
\left[\begin{matrix}1 &amp;amp; 0 \\ 0 &amp;amp; 1 \end{matrix}\right]
\left[\begin{matrix}\eta_{\mu, t}\\\eta_{\delta, t}\end{matrix}\right],
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z_t = (1,0)^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t = (\mu_t, \delta_t)^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_t = \left[\begin{smallmatrix}1 &amp;amp; 1\\ 0 &amp;amp; 1 \end{smallmatrix}\right]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(R_t=\left[\begin{smallmatrix}1 &amp;amp; 0 \\ 0 &amp;amp; 1 \end{smallmatrix}\right]\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\eta_t = (\eta_{\mu,t}, \eta_{\delta, t})^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddLocalLinearTrend(ss, y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;semi-local-linear-trend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Semi-local linear trend&lt;/h3&gt;
&lt;p&gt;The semi-local linear trend is similar to the local linear trend, but more useful for long-term forecasting. The model can be written as &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = \mu_t +\epsilon_t, \\
&amp;amp; \mu_{t+1} = \mu_{t} + \delta_t + \eta_{\mu,t}, \qquad \text{(stochastic level component)}\\
&amp;amp; \delta_{t+1} = a + \phi\times (\delta_t-a) + \eta_{\delta,t}, \qquad \text{(stochastic slope component)}\\
&amp;amp; \eta_{\mu, t} \sim N(0,\sigma^2_{\mu, t}),\quad
  \eta_{\delta, t} \sim N(0,\sigma^2_{\delta, t})
\end{split}\]&lt;/span&gt; where the slope component &lt;span class=&#34;math inline&#34;&gt;\(\delta_{t+1}\)&lt;/span&gt; is modeled by an AR(1) process centered at value &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. A stationary AR(1) process is less variable than a random walk so it often gives more reasonable uncertainty estimates when making long term forecasts.&lt;/p&gt;
&lt;p&gt;The model can be expressed in the general form with &lt;span class=&#34;math inline&#34;&gt;\(Z_t = (1,0, a)^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t = (\mu_t+a, \delta_t-a, -1)^T\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_t = \left[\begin{smallmatrix}1 &amp;amp; 1 &amp;amp; 0 \\ 0 &amp;amp; \phi &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 1 \end{smallmatrix}\right]\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(R_t=\left[\begin{smallmatrix}1 &amp;amp; 0 \\ 0 &amp;amp; 1\\0&amp;amp;0 \end{smallmatrix}\right]\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\eta_t = (\eta_{\mu,t}, \eta_{\delta, t})^T\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddSemilocalLinearTrend(ss, y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Seasonality&lt;/h2&gt;
&lt;div id=&#34;regression-with-seasonal-dummy-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression with Seasonal Dummy Variables&lt;/h3&gt;
&lt;p&gt;There are several commonly used state-component models to capture seasonality. The most frequently used model in the time domain is &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = \gamma_t +\epsilon_t, \\
&amp;amp; \gamma_{t+d} = - \sum_{i=0}^{s-2}\gamma_{t-i\times d} + \eta_{\gamma, t},
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is the number of seasons and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is the seasonal duration (number of time periods in each season, often set to 1). The model can be thought of as a regression on &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; dummy variables representing &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; seasons and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{t}\)&lt;/span&gt; denotes their joint contribution to the observed response &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt;. The mean of &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{t+d}\)&lt;/span&gt; is such that the total seasonal effect is zero when summed over &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; seasons (i.e. &lt;span class=&#34;math inline&#34;&gt;\(E(\gamma_{t+d}+\sum_{i=0}^{s-2}\gamma_{t-i\times d}) = 0\)&lt;/span&gt;). The model can be rewritten as &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = [1\quad 0 \quad \cdots\quad 0]\left[\begin{matrix}\gamma_{t}\\\gamma_{t-d}\\ \vdots\\
\gamma_{t-(s-2)d}\end{matrix}\right] +\epsilon_t, \\
&amp;amp; \left[\begin{matrix}\gamma_{t+d}\\\gamma_t\\\gamma_{t-d}\\ \vdots\\
\gamma_{t-(s-4)d}\\
\gamma_{t-(s-3)d}\end{matrix}\right] = 
\left[\begin{matrix} -1 &amp;amp; - 1 &amp;amp; \cdots &amp;amp; -1 &amp;amp; -1 \\ 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp;0&amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp;0 \\
\vdots &amp;amp;\vdots &amp;amp;\vdots &amp;amp;\vdots &amp;amp;\vdots &amp;amp;\\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
\end{matrix}\right]
\left[\begin{matrix}\gamma_{t}\\\gamma_{t-d}\\\gamma_{t-2d}\\\vdots \\
\gamma_{t-(s-3)d}\\
\gamma_{t-(s-2)d}\end{matrix}\right] + \left[\begin{matrix}1\\0\\0\\ \vdots\\
0\\
0\end{matrix}\right]\eta_{\gamma, t}
\end{split}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The seasonal model can be generalized to allow for multiple seasonal components with different periods.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
# Suppose that y is a time series collected hourly
ss &amp;lt;- list()
# daily seasonality
ss &amp;lt;- bsts::AddSeasonal(ss, y, nseasons = 24, season.duration = 1) 
# weely seasonality
ss &amp;lt;- bsts::AddSeasonal(ss, y, nseasons = 7, season.duration = 24)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trigonometric-seasonal-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Trigonometric Seasonal model&lt;/h3&gt;
&lt;p&gt;Another way to model seasonality is to use trigonometric seasonal model, &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; y_t = \gamma_t +\epsilon_t, \\
&amp;amp; \gamma_{j, t+1} = \gamma_{j,t}\times \cos(\lambda_j) - \gamma^*_{j,t}\times \sin(\lambda_j) + \omega_{j, t},\\
&amp;amp; \gamma^*_{j, t+1} = \gamma^*_{j,t}\times \cos(\lambda_j) - \gamma_{j,t}\times \sin(\lambda_j) + \omega^*_{j,t},\\
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\gamma_t = \sum_{j=1}^{k}\gamma_{j,t}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_j = 2\pi j/s\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th seasonal frequency, &lt;span class=&#34;math inline&#34;&gt;\(j = 1,\cdots, k\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is the number of time steps required for the longest cycle to repeat (i.e. number of seasosns).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
# The harmonic method is strongly preferred to the direct method.
# For a time series collected hourly with daily 
# seasonality, we can try 
# the component below, where frequencies = 1:4 specifies the frequencis of sin, cos functions we will use.
ss &amp;lt;- bsts::AddTrig(ss, y, period = 24, frequencies = 1:4, method = &amp;quot;harmonic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear regression&lt;/h2&gt;
&lt;p&gt;The covariates in structural time series models are assumed to be contemporaneous. The coefficients of the contemporaneous covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_t\)&lt;/span&gt; can be static or time-varying.&lt;/p&gt;
&lt;p&gt;A static regression can be written in state-space form by setting &lt;span class=&#34;math inline&#34;&gt;\(Z_t = \beta^T \mathbf{x}_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t =1\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is static.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
bsts::bsts(y ~ x, ss, niter = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A dynamic regression component can be written as &lt;span class=&#34;math display&#34;&gt;\[\begin{split}
&amp;amp; \mathbf{x}_t^T\beta_t = \sum_{j=1}^J x_{j,t} \beta_{j,t}\\
&amp;amp; \beta_{j, t+1} = \beta_{j,t} + \eta_{\beta,j,t},
\end{split}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,t}\)&lt;/span&gt; is the coefficient for the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th covariate &lt;span class=&#34;math inline&#34;&gt;\(x_{j,t}\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. The state-space form is given as &lt;span class=&#34;math inline&#34;&gt;\(Z_t = \mathbf{x}_t, \alpha_t =\beta_t, T_t = R_t = I_{J\times J}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddDynamicRegression(ss, y ~ x)
bsts::bsts(y, ss, niter = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;assemble-multiple-state-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assemble multiple state components&lt;/h2&gt;
&lt;p&gt;Independent state components can be combined by concatenating their observation vectors &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; and arranging the other model matrices as elements in a block diagonal matrix. For example, we can combine the &lt;em&gt;local linear trend&lt;/em&gt; with &lt;em&gt;seasonality&lt;/em&gt; and have the following model matrices: &lt;span class=&#34;math display&#34;&gt;\[Z_t = \left[\begin{smallmatrix}
1  \\ 
0 \\
1 \\
0 \\
\vdots\\
0\end{smallmatrix}\right], 
T_t = \left[\begin{smallmatrix}
  1 &amp;amp; 1 &amp;amp; \\ 
  0 &amp;amp; 1 &amp;amp; \\
    &amp;amp;   &amp;amp; -1 &amp;amp; - 1 &amp;amp; \cdots &amp;amp; -1 &amp;amp; -1 \\ 
    &amp;amp;   &amp;amp; 1 &amp;amp; 0 &amp;amp; \cdots &amp;amp;0&amp;amp; 0\\
    &amp;amp;   &amp;amp; 0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp;0 \\
    &amp;amp;   &amp;amp; \vdots &amp;amp;\vdots &amp;amp;\vdots &amp;amp;\vdots &amp;amp;\vdots &amp;amp;\\
    &amp;amp;   &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 \\
    &amp;amp;   &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
  \end{smallmatrix}\right],
R_t=\left[\begin{smallmatrix}
1 &amp;amp; 0 \\ 
0 &amp;amp; 1 \\
  &amp;amp;  &amp;amp; 1 \\
  &amp;amp; &amp;amp;  0 \\
    &amp;amp; &amp;amp;  \vdots \\
    &amp;amp; &amp;amp;  0 \\
\end{smallmatrix}\right] \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddLocalLinearTrend(ss, y)
ss &amp;lt;- bsts::AddSeasonality(ss, y, nseason = s, season.duration = d)
bsts::bsts(y, ss, niter = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, we can add other state components such as Regression, Local level, AR process, Random walk for holiday effect, etc (see &lt;em&gt;bsts&lt;/em&gt; Package for more details). As mentioned in the Introduction, the model is modular and can be easily extended by using the &lt;em&gt;bsts&lt;/em&gt; package. For example,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R code
ss &amp;lt;- list()
ss &amp;lt;- bsts::AddLocalLevel(ss, y)
ss &amp;lt;- bsts::AddAr(ss, y)
ss &amp;lt;- bsts::AddSeasonal(ss, y, nseasons = 24)
ss &amp;lt;- bsts::AddDynamicRegression(ss, y ~ x1)
bsts::bsts(y ~ x2, ss, niter = 500)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some examples of application to real data analyses can be found &lt;a href=&#34;http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-diagnostic&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model diagnostic&lt;/h1&gt;
&lt;div id=&#34;prediction-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction errors&lt;/h2&gt;
&lt;p&gt;As part of the model fitting process, the algorithm in &lt;em&gt;bsts&lt;/em&gt; generates the one-step-ahead prediction errors &lt;span class=&#34;math inline&#34;&gt;\(y_tbE(y_t|Y_{tb1},\theta)\)&lt;/span&gt; , where &lt;span class=&#34;math inline&#34;&gt;\(Y_{tb1}=y_1,\cdots,y_{tb1}\)&lt;/span&gt;, and the vector of model parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is fixed at its current value in the MCMC algorithm. The one-step-ahead prediction errors can be obtained from the bsts model by calling&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bsts.prediction.errors(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The one step prediction errors are a useful diagnostic for comparing several bsts models that have been fit to the same data. They are used to implement the function &lt;code&gt;CompareBstsModels&lt;/code&gt;, which is called as shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CompareBstsModels(list(&amp;quot;Model 1&amp;quot; = model1,
                       &amp;quot;Model 2&amp;quot; = model2,
                       &amp;quot;Model 3&amp;quot; = model3),
                  colors = c(&amp;quot;black&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-assumptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model assumptions&lt;/h2&gt;
&lt;p&gt;In BSTS models, the residual term is often assumed to follow a Gaussian distribution. It is important to check the validity of such assumption to see if the model is valid or not.&lt;/p&gt;
&lt;p&gt;The output of bsts model contains a matrix of Monte Carlo draws of residual errors. Each row is a Monte Carlo draw, and each column is an observation.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;qqdist&lt;/code&gt; function sorts the columns of draws by their mean, and plots the resulting set of curves against the quantiles of the standard normal distribution. A reference line is added, and the mean of each column of draws is represented by a blue dot. If the dots fall around the straight line, the normality assumption holds well for the residuals. If the dots depart greatly from the line, the normality assumption may violate. In this case, we may either do data transformation to obtain more normally distributed data or assume a different distribution on the data (e.g. t-distribution)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;AcfDist&lt;/code&gt; function plots the posterior distribution of the autocorrelation function (ACF) of the residuals using a set of side-by-side boxplots. If the boxplot of lag &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; contains 0, we may consider the residuals are uncorrelated at lag &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. If the ACF does not dampen out (i.e. falling to zero) within about 15 to 20 lags, the residual term is nonstationary and we should try a different model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Steven L. Scott. (2017). Fitting Bayesian structural time series with the bsts R package. &lt;a href=&#34;http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html&#34; class=&#34;uri&#34;&gt;http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Chatfield, Chris. (2016). The analysis of time series: an introduction. CRC press.&lt;/p&gt;
&lt;p&gt;Montgomery, Douglas C., Cheryl L. Jennings, and Murat Kulahci. (2015). Introduction to time series analysis and forecasting. John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/publication/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/publication/publications/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
